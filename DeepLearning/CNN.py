'''
This tensorflow code is based on the example of Burak Himmetoglu liskted on https://burakhimmetoglu.com/2017/08/22/time-series-classification-with-tensorflow/,
the origin code is listed here: https://github.com/healthDataScience/deep-learning-HAR
The model has been modified and tuned to fit our project 
The input of the network are raw time series data of all the sensors used for machine learing (EMG1, EMG2, Vib1, Vib2, Acc). They are divided to sequence length of 100 (can be modified).
Each sensor has its only data file with each line conisits of 100 data. These inputs files are generated by the dataconversion.py
'''

# Imports
import numpy as np
import os
from utils.utilities import *
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt


# Prepare data


X_train, labels_train, list_ch_train = read_data(data_path="./data/", split="train") # train
X_test, labels_test, list_ch_test = read_data(data_path="./data/", split="test") # test

#assert list_ch_train == list_ch_test, "Mistmatch in channels!"


# Train/Validation Split


X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, 
                                                stratify = labels_train, random_state = 183)


# One-hot encoding:



y_tr = one_hot(lab_tr)
y_vld = one_hot(lab_vld)
y_test = one_hot(labels_test)


# Imports
import tensorflow as tf


# Hyperparameters


batch_size = 200       # Batch size(in utils.py as well ,this needs to be smaller than number of test data samples to do the test(<20)
seq_len = 100          # Number of steps
learning_rate = 0.0001#0.0001
epochs = 1000

n_classes = 2
n_channels = 10


# ### Construct the graph

graph = tf.Graph()

# Construct placeholders
with graph.as_default():
    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')
    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')
    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')
    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')


# Build Convolutional Layers

with graph.as_default():
    # (batch, 100, 10) --> (batch, 50, 20)
    conv1 = tf.layers.conv1d(inputs=inputs_, filters=20, kernel_size=2, strides=1, 
                             padding='same', activation = tf.nn.relu)#100,20
    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='valid')#50,20
    
    # (batch, 50, 20) --> (batch, 24, 40)
    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=40, kernel_size=2, strides=1, 
                             padding='same', activation = tf.nn.relu)#50,40
    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='valid')#24,40?
    
    # (batch, 24, 40) --> (batch, 12, 80)
    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=80, kernel_size=2, strides=1, 
                             padding='same', activation = tf.nn.relu)#24,80
    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='valid')#12,80
    
    # (batch, 12, 80) --> (batch, 6, 160)
    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=160, kernel_size=2, strides=1, 
                             padding='same', activation = tf.nn.softmax)#12,160
    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='valid')#6,160


# Now, flatten and pass to the classifier



with graph.as_default():
    # Flatten and add dropout
    flat = tf.reshape(max_pool_4, (-1,6*160))
    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)
    
   # Predictions
    logits = tf.layers.dense(flat, n_classes)
    # specify some class weightings
    class_weights = tf.constant([1, 4.0])#class weights to deal with data imblance

    # specify the weights for each sample in the batch (without having to compute the onehot label matrix)
    #weights = tf.gather(class_weights, labels_)
    weights = tf.reduce_sum(class_weights * labels_, axis=1)
    print(weights)
    
    # Cost function and optimizer
    unweightedloss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_) #should use softmax for classification problems
    weightedloss = unweightedloss * tf.cast(weights,tf.float32)
    cost = tf.reduce_mean(weightedloss)
    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)
    # Accuracy
    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')


# ### Train the network


validation_acc = []
validation_loss = []

train_acc = []
train_loss = []

with graph.as_default():
    saver = tf.train.Saver()

with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    iteration = 1
   
    # Loop over epochs
    for e in range(epochs):
        
        # Loop over batches
        for x,y in get_batches(X_tr, y_tr, batch_size):
            
            # Feed dictionary
            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, learning_rate_ : learning_rate}
            
            # Loss
            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)
            train_acc.append(acc)
            train_loss.append(loss)
            
            # Print at each 5 iters
            if (iteration % 5 == 0):
                print("Epoch: {}/{}".format(e, epochs),
                      "Iteration: {:d}".format(iteration),
                      "Train loss: {:6f}".format(loss),
                      "Train acc: {:.6f}".format(acc))
            
            # Compute validation loss at every 10 iterations
            if (iteration%10 == 0):                
                val_acc_ = []
                val_loss_ = []
                
                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):
                    # Feed
                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0}  
                    
                    # Loss
                    loss_v, acc_v = sess.run([cost, accuracy], feed_dict = feed)                    
                    val_acc_.append(acc_v)
                    val_loss_.append(loss_v)
                
                # Print info
                print("Epoch: {}/{}".format(e, epochs),
                      "Iteration: {:d}".format(iteration),
                      "Validation loss: {:6f}".format(np.mean(val_loss_)),
                      "Validation acc: {:.6f}".format(np.mean(val_acc_)))
                
                # Store
                validation_acc.append(np.mean(val_acc_))
                validation_loss.append(np.mean(val_loss_))
            
            # Iterate 
            iteration += 1
    
    saver.save(sess,"checkpoints-cnn/har.ckpt")



# Plot training and test loss
t = np.arange(iteration-1)

plt.figure(figsize = (6,6))
plt.plot(t, np.array(train_loss), 'r-', t[t % 10 == 0], np.array(validation_loss), 'b*')
plt.xlabel("iteration")
plt.ylabel("Loss")
plt.legend(['train', 'validation'], loc='upper right')
plt.show()


# Plot Accuracies
plt.figure(figsize = (6,6))

plt.plot(t, np.array(train_acc), 'r-', t[t % 10 == 0], validation_acc, 'b*')
plt.xlabel("iteration")
plt.ylabel("Accuray")
plt.legend(['train', 'validation'], loc='upper right')
plt.show()


# ## Evaluate on test set


test_acc = []
test_pred = np.array([])

with tf.Session(graph=graph) as sess:
    # Restore
    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-cnn'))
    x_t_tot = []
    i = 0
    test_counter = 0

    totalcoughacc = 0
    for x_t, y_t in get_batches(X_test[:-1], y_test[:-1], batch_size):
        onecount=0
        cp=0
        feed = {inputs_: x_t,
                labels_: y_t,
                keep_prob_: 1}
        
        batch_acc = sess.run(accuracy, feed_dict=feed)
        test_counter = test_counter + 1
        test_acc.append(batch_acc)

        prediction = tf.argmax(logits, 1)
        current_pred = prediction.eval(feed_dict=feed, session=sess)
        correct=tf.argmax(y_t, 1)
        correct_label=correct.eval(feed_dict=feed, session=sess)
        print("predicted: ")
        print(current_pred)
        print("correct: ")
        print(correct_label)
        print("one batch finished")
        for j in range(0, batch_size):
            if correct_label[j] == 1:
                if current_pred[j] == 1:
                    cp = cp + 1
                onecount = onecount+1
        coughacc = cp/onecount
        totalcoughacc = totalcoughacc+coughacc
        print("batch cough accuracy: ")
        print(coughacc)
        #print(y_test)
       # test_pred = np.append(test_pred, current_pred)
        if(i==0):
            x_t_tot = x_t
        else:
            x_t_tot = np.concatenate((x_t_tot, x_t), axis=0)
        i += 1
    avgcoughacc = totalcoughacc/i
    #print(sess.run(logits, feed_dict=feed))
    print("Test accuracy: {:.6f}".format(np.mean(test_acc)))
   # output_graph(test_pred, x_t_tot)
    print("done")
